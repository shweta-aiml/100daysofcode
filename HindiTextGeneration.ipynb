{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HindiTextGeneration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqUOQtDJlNBjAazZQJAwhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shweta-aiml/100daysofcode/blob/master/HindiTextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f4Cmx_dEFrU",
        "colab_type": "code",
        "outputId": "5d295dbe-d5c7-4bf5-91b7-ba448b30c5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install indic-transliteration "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-transliteration\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/73/0491e03996146fc605f2cacf751968f35cfc586aadd9e5181fa47b0f9bdf/indic_transliteration-1.9.4-py3-none-any.whl (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 1.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 30kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 51kB 1.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 61kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 71kB 1.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 81kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 92kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from indic-transliteration) (2019.12.20)\n",
            "Collecting splinter\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/46/bb01079ef246d61c2432420c6cd63ecb11e0e909a5da42abcb407e0fb4e2/splinter-0.13.0.tar.gz\n",
            "Collecting backports.functools-lru-cache\n",
            "  Downloading https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl\n",
            "Collecting selenium>=3.141.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from splinter->indic-transliteration) (1.12.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.141.0->splinter->indic-transliteration) (1.24.3)\n",
            "Building wheels for collected packages: splinter\n",
            "  Building wheel for splinter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for splinter: filename=splinter-0.13.0-cp36-none-any.whl size=33304 sha256=02bb37118811f2f4c004a1d81876d727969c0adc3ef6d8ff034083a7e2447837\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/ff/26/2fc56897bcbe58908bbb2002c53affb516e5e6daa425aefdef\n",
            "Successfully built splinter\n",
            "Installing collected packages: selenium, splinter, backports.functools-lru-cache, indic-transliteration\n",
            "Successfully installed backports.functools-lru-cache-1.6.1 indic-transliteration-1.9.4 selenium-3.141.0 splinter-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVys5Nv5FuA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "from indic_transliteration import sanscript\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL0txi_1F9i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the data , check to see if we can load data from one file lets say premchandra \n",
        "#create input and putput sequence of data\n",
        "#creathe the model \n",
        "#train\n",
        "#predict \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YD6JmGbpozJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path where the corpus will end up.\n",
        "corpus_path = \"corpus\"\n",
        "file_contents = []\n",
        "# Hyperparameters.\n",
        "transliteration = True # Transliterates the corpus.\n",
        "input_length = 40 # Length of the input sequence.\n",
        "output_length = 1 # Length of the output sequence.\n",
        "data_set_size = 100000 # Size of the data-set to train on.\n",
        "num_epochs = 50 # Number of epochs to train.\n",
        "batch_size = 512 # Batch size during training.\n",
        "hidden_size = 350 # Size of the hidden layer.\n",
        "generation_length = 160 # Size of the strings that are generated.\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B0bYKR2sVD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downloading corpus from the github sites\n",
        "def ensure_corpus():\n",
        "    \"\"\" Makes sure that the corpus is on the hard-drive.\"\"\"\n",
        "\n",
        "    # Do nothing if the filder already exists-\n",
        "    if os.path.exists(\"corpus\") == False:\n",
        "        # Download the whole git-repository as a zip.\n",
        "        print(\"Downloading corpus...\")\n",
        "        corpus_url = \"https://github.com/cltk/hindi_text_ltrc/archive/master.zip\"\n",
        "        corpus_zip_path = \"master.zip\"\n",
        "        urllib.request.urlretrieve(corpus_url, corpus_zip_path)\n",
        "\n",
        "        # Unzip the whole git-repository to the corpus-path.\n",
        "        print(\"Unzipping corpus...\")\n",
        "        zip_file = zipfile.ZipFile(corpus_zip_path, 'r')\n",
        "        zip_file.extractall(corpus_path)\n",
        "        zip_file.close()\n",
        "\n",
        "        # Remove the zip-file.\n",
        "        os.remove(corpus_zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCv7juFaKutJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file():\n",
        "    # Get paths to all files.\n",
        "    glob_path = os.path.join(corpus_path, \"**/*.txt\")\n",
        "    paths = glob.glob(glob_path, recursive=True)\n",
        "    print(\"Display all text files \")\n",
        "    print(paths)\n",
        "    return paths\n",
        "\n",
        "def load_file(paths):\n",
        "    print(\"Loading all files...\")\n",
        "    \n",
        "    for path in paths:\n",
        "        file_content = open(path, \"r\").read()\n",
        "        if transliteration == True:\n",
        "            file_content = sanscript.transliterate(file_content, sanscript.DEVANAGARI, sanscript.IAST)\n",
        "        file_content = clean_text(file_content)\n",
        "        file_contents.append(file_content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzeiyjfDJrtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unique list of characters\n",
        "def get_character_set(text):\n",
        "    \"\"\" Retrieves the unique set of characters. \"\"\"\n",
        "    vocab =  sorted(list(set(text)))\n",
        "    print ('{} unique characters'.format(len(vocab)))\n",
        "    return vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIA0F11LQRrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\" Cleans a text. \"\"\"\n",
        "\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"।\", \" \")\n",
        "    text = text.replace(\"0\", \" \")\n",
        "    text = text.replace(\"1\", \" \")\n",
        "    text = text.replace(\"2\", \" \")\n",
        "    text = text.replace(\"3\", \" \")\n",
        "    text = text.replace(\"4\", \" \")\n",
        "    text = text.replace(\"5\", \" \")\n",
        "    text = text.replace(\"6\", \" \")\n",
        "    text = text.replace(\"7\", \" \")\n",
        "    text = text.replace(\"8\", \" \")\n",
        "    text = text.replace(\"9\", \" \")\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xBNrdVKWF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(vocab,text):\n",
        "  #vectorizing the text\n",
        "  # Creating a mapping from unique characters to indices\n",
        "  char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "  idx2char = np.array(vocab)\n",
        "\n",
        "  text_as_int = np.array([char2idx[c] for c in text])\n",
        "  print('{')\n",
        "  for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "  print('  ...\\n}')\n",
        "  # Show how the first 13 characters from the text are mapped to integers\n",
        "  print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "  return text_as_int,idx2char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4uhtHmOSPWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "def char_to_seq(text,text_as_int):\n",
        "  seq_length = 100\n",
        "  examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "  # Create training examples / targets\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "  for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEi-A-eMTwEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "4598f335-9dfb-4f2f-caca-26d97bf37969"
      },
      "source": [
        "ensure_corpus()\n",
        "paths = read_file()\n",
        "load_file(paths)\n",
        " # Getting character set.\n",
        "print(\"Getting character set...\")\n",
        "global full_text\n",
        "full_text = \" \".join(file_contents)\n",
        "global character_set\n",
        "character_set = get_character_set(full_text)\n",
        "print(\"Character set:\", character_set, len(character_set))\n",
        "text_int,idx2char = vectorize_text(character_set,full_text)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display all text files \n",
            "['corpus/hindi_text_ltrc-master/Rahima/main.txt', 'corpus/hindi_text_ltrc-master/Kabeera/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/Uddhav/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/gandhi/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/skgt/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/kamayani/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/vaktavya/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/seva/main.txt', 'corpus/hindi_text_ltrc-master/cakra/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/KV/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/5/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/6/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/1/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/2/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/3/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/7/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/4/main.txt', 'corpus/hindi_text_ltrc-master/Meera/bjnm/main.txt', 'corpus/hindi_text_ltrc-master/Meera/bjn/main.txt']\n",
            "Loading all files...\n",
            "Getting character set...\n",
            "73 unique characters\n",
            "Character set: [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', ':', '=', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'y', 'z', '|', 'Ê', 'Ù', 'Ý', 'è', 'é', 'ê', 'í', 'ñ', 'û', 'ā', 'ġ', 'ī', 'ś', 'ū', '̐', '़', 'ॅ', 'ॉ', 'फ़', 'ḍ', 'ḥ', 'ḷ', 'ḻ', 'ṃ', 'ṅ', 'ṇ', 'ṛ', 'ṝ', 'ṣ', 'ṭ', '\\u200c', '\\u200d', '–', '’'] 73\n",
            "{\n",
            "  ' ' :   0,\n",
            "  '!' :   1,\n",
            "  '\"' :   2,\n",
            "  \"'\" :   3,\n",
            "  '(' :   4,\n",
            "  ')' :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '/' :   9,\n",
            "  ':' :  10,\n",
            "  '=' :  11,\n",
            "  '?' :  12,\n",
            "  '`' :  13,\n",
            "  'a' :  14,\n",
            "  'b' :  15,\n",
            "  'c' :  16,\n",
            "  'd' :  17,\n",
            "  'e' :  18,\n",
            "  'f' :  19,\n",
            "  ...\n",
            "}\n",
            "'uttama jāti h' ---- characters mapped to int ---- > [34 33 33 14 26 14  0 23 48 33 22  0 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkElQulCU12D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "03e08ec4-4b8c-4f70-e1b1-4d707f502583"
      },
      "source": [
        "text_int,idx2char"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([34, 33, 33, ..., 27, 48, 38]),\n",
              " array([' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', ':', '=', '?',\n",
              "        '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
              "        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'y', 'z', '|',\n",
              "        'Ê', 'Ù', 'Ý', 'è', 'é', 'ê', 'í', 'ñ', 'û', 'ā', 'ġ', 'ī', 'ś',\n",
              "        'ū', '̐', '़', 'ॅ', 'ॉ', 'फ़', 'ḍ', 'ḥ', 'ḷ', 'ḻ', 'ṃ', 'ṅ', 'ṇ',\n",
              "        'ṛ', 'ṝ', 'ṣ', 'ṭ', '\\u200c', '\\u200d', '–', '’'], dtype='<U1'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN8QZhEqOix4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7ab843c6-7b03-49b7-c35c-dc41563a3fb1"
      },
      "source": [
        "print((text_int))\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(full_text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34 33 33 ... 27 48 38]\n",
            "u\n",
            "t\n",
            "t\n",
            "a\n",
            "m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6KUYvHvVP2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "eefca8ba-3309-4627-eef0-1b24583eaf31"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'uttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala meṃ'\n",
            "' harata, parasata vāke pāya rūparaṃga ratirāja meṃ, chatarānī itarāna| mānau racī biraṃci paci, kusum'\n",
            "'a kanaka meṃ sāna baniyāini bani āikai, baiṭhi rūpa kī hāṭa| pema peka tana herikai, garuvai ṭārati b'\n",
            "'āṭa garaba tarājū karati cakha, bhauṃha mori musakāti| ḍām̐fī mārati biraha kī, cita ciṃtā ghaṭi jāti'\n",
            "' kamala-dala nainani kī unamāni / rahīma kamala-dala nainani kī unamāni| bisarata nāhiṃ sakhī mo mana'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt7soWYMVAYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPS02POgVVFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f27b42da-d56d-468d-c409-f669b257a936"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'uttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala me'\n",
            "Target data: 'ttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala meṃ'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk0BwKToVis6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea3cf599-765f-4f27-bfe8-a619dfd2dd63"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0srsOmu8VoXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(character_set)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC83vIZwWH4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Df6nq0WKdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(character_set),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ4mR7iaWP4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5d795ab-c02a-4c8e-b2d9-ca453dcfce02"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 73) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfUrNjGrWSsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e032777a-08fc-47a3-d804-6e73cfed14e3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           18688     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 73)            74825     \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_rniZB_WVQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I29PLE0gWXfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "206244f0-90bf-4e99-84c1-8e1fe810e8d8"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11, 31, 55, 59, 58, 35, 28, 10, 49,  3, 67,  2, 52, 44, 42, 41, 71,\n",
              "       56, 33, 32, 60,  6,  7, 49, 37, 59,  4,  3, 64, 10,  9, 67, 34, 54,\n",
              "       61, 64, 33, 40, 23, 15, 23,  9, 60, 60, 45, 50,  5, 37, 50, 24, 50,\n",
              "       24, 68, 70, 52, 17,  3, 45, 38, 45, 25, 67, 62, 16, 41, 42, 59, 54,\n",
              "       61, 10, 24, 59, 70, 53, 51, 37,  4, 29, 69, 47, 19,  3, 15, 43, 42,\n",
              "       52, 39, 57, 59, 66, 58, 10, 12, 46, 24,  6,  0,  6, 10, 40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO31QFlNWaCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e55aad67-0983-4e0a-e7b9-1f1aaed69b22"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "temp = ''.join(idx2char[sampled_indices])\n",
        "new_text = sanscript.transliterate(temp, sanscript.DEVANAGARI, sanscript.IAST)\n",
        "print(new_text)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"ṛgayā kā śithila huā na karma ' tisa para yaha pīlāpana kaisā r yaha kyoṃ bunane kā śrama sakheda ṛ \"\n",
            "\n",
            "Next Char Predictions: \n",
            "\n",
            "=rॅḥḍvo:ġ'ṣ\"ūêèÝ–ॉtsḷ,-ġzḥ('ṇ:/ṣu़ḻṇtÙjbj/ḷḷíī)zīkīkṭ‍ūd'í|ílṣṃcÝèḥ़ḻ:kḥ‍̐śz(p‌ûf'béèūÊफ़ḥṝḍ:?ñk, ,:Ù\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwocxBEgYBoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "27cbe4cf-d5dc-4e3a-c69a-8d328a6d8235"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 73)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.2906556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CncARvwBYEMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fT6a4eYG3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU3cMW6WYJbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e6792e2-a87b-4ecf-8368-293c74786b96"
      },
      "source": [
        "EPOCHS=10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  91/5927 [..............................] - ETA: 7:06:18 - loss: 2.8804"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XLc4MgJZLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    \"\"\" Loads the data from the corpus. \"\"\"\n",
        "\n",
        "\n",
        "    # Getting character set.\n",
        "    print(\"Getting character set...\")\n",
        "    global full_text\n",
        "    full_text = \" \".join(file_contents)\n",
        "    global character_set\n",
        "    character_set = get_character_set(full_text)\n",
        "    print(\"Character set:\", character_set, len(character_set))\n",
        "\n",
        "    # Process the data.\n",
        "    data_input = []\n",
        "    data_output = []\n",
        "    current_size = 0\n",
        "    print(\"Generating data set...\")\n",
        "    bar = progressbar.ProgressBar(max_value=data_set_size)\n",
        "    while current_size < data_set_size:\n",
        "        random_file_content = random.choice(file_contents)\n",
        "\n",
        "        random_string = random_substring_of_length(random_file_content, input_length + output_length)\n",
        "\n",
        "        random_string_encoded = encode_string(random_string)\n",
        "\n",
        "        input_sequence = random_string_encoded[:input_length]\n",
        "        output_sequence = random_string_encoded[input_length:]\n",
        "\n",
        "        data_input.append(input_sequence)\n",
        "        data_output.append(output_sequence)\n",
        "\n",
        "        current_size += 1\n",
        "        bar.update(current_size)\n",
        "    bar.finish()\n",
        "\n",
        "    # Done.\n",
        "    train_input = np.array(data_input)\n",
        "    train_output = np.array(data_output)\n",
        "    return (train_input, train_output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}