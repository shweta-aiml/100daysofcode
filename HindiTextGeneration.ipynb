{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HindiTextGeneration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuk9Sx/U1G+DXXRqYvxj8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shweta-aiml/100daysofcode/blob/master/HindiTextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f4Cmx_dEFrU",
        "colab_type": "code",
        "outputId": "d6789daf-ebe4-4e26-f6de-a88fa285efd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install indic-transliteration "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-transliteration\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/73/0491e03996146fc605f2cacf751968f35cfc586aadd9e5181fa47b0f9bdf/indic_transliteration-1.9.4-py3-none-any.whl (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 1.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 30kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 51kB 1.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 61kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 71kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 81kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 92kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from indic-transliteration) (2019.12.20)\n",
            "Collecting splinter\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/46/bb01079ef246d61c2432420c6cd63ecb11e0e909a5da42abcb407e0fb4e2/splinter-0.13.0.tar.gz\n",
            "Collecting backports.functools-lru-cache\n",
            "  Downloading https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl\n",
            "Collecting selenium>=3.141.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from splinter->indic-transliteration) (1.12.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.141.0->splinter->indic-transliteration) (1.24.3)\n",
            "Building wheels for collected packages: splinter\n",
            "  Building wheel for splinter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for splinter: filename=splinter-0.13.0-cp36-none-any.whl size=33304 sha256=0ec0af8d13815ca4a0f0d5c3e366dee433add495725bd5d09affd576a0845ede\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/ff/26/2fc56897bcbe58908bbb2002c53affb516e5e6daa425aefdef\n",
            "Successfully built splinter\n",
            "Installing collected packages: selenium, splinter, backports.functools-lru-cache, indic-transliteration\n",
            "Successfully installed backports.functools-lru-cache-1.6.1 indic-transliteration-1.9.4 selenium-3.141.0 splinter-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVys5Nv5FuA9",
        "colab_type": "code",
        "outputId": "d8f88354-9ffe-4a98-db03-48d476825619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "from indic_transliteration import sanscript\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL0txi_1F9i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the data , check to see if we can load data from one file lets say premchandra \n",
        "#create input and putput sequence of data\n",
        "#creathe the model \n",
        "#train\n",
        "#predict \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YD6JmGbpozJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path where the corpus will end up.\n",
        "corpus_path = \"corpus\"\n",
        "file_contents = []\n",
        "# Hyperparameters.\n",
        "transliteration = True # Transliterates the corpus.\n",
        "input_length = 40 # Length of the input sequence.\n",
        "output_length = 1 # Length of the output sequence.\n",
        "data_set_size = 100000 # Size of the data-set to train on.\n",
        "num_epochs = 50 # Number of epochs to train.\n",
        "batch_size = 512 # Batch size during training.\n",
        "hidden_size = 350 # Size of the hidden layer.\n",
        "generation_length = 160 # Size of the strings that are generated.\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B0bYKR2sVD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downloading corpus from the github sites\n",
        "def ensure_corpus():\n",
        "    \"\"\" Makes sure that the corpus is on the hard-drive.\"\"\"\n",
        "\n",
        "    # Do nothing if the filder already exists-\n",
        "    if os.path.exists(\"corpus\") == False:\n",
        "        # Download the whole git-repository as a zip.\n",
        "        print(\"Downloading corpus...\")\n",
        "        corpus_url = \"https://github.com/cltk/hindi_text_ltrc/archive/master.zip\"\n",
        "        corpus_zip_path = \"master.zip\"\n",
        "        urllib.request.urlretrieve(corpus_url, corpus_zip_path)\n",
        "\n",
        "        # Unzip the whole git-repository to the corpus-path.\n",
        "        print(\"Unzipping corpus...\")\n",
        "        zip_file = zipfile.ZipFile(corpus_zip_path, 'r')\n",
        "        zip_file.extractall(corpus_path)\n",
        "        zip_file.close()\n",
        "\n",
        "        # Remove the zip-file.\n",
        "        os.remove(corpus_zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCv7juFaKutJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file():\n",
        "    # Get paths to all files.\n",
        "    glob_path = os.path.join(corpus_path, \"**/*.txt\")\n",
        "    paths = glob.glob(glob_path, recursive=True)\n",
        "    print(\"Display all text files \")\n",
        "    print(paths)\n",
        "    return paths\n",
        "\n",
        "def load_file(paths):\n",
        "    print(\"Loading all files...\")\n",
        "    \n",
        "    for path in paths:\n",
        "        file_content = open(path, \"r\").read()\n",
        "        if transliteration == True:\n",
        "            file_content = sanscript.transliterate(file_content, sanscript.DEVANAGARI, sanscript.IAST)\n",
        "        file_content = clean_text(file_content)\n",
        "        file_contents.append(file_content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzeiyjfDJrtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unique list of characters\n",
        "def get_character_set(text):\n",
        "    \"\"\" Retrieves the unique set of characters. \"\"\"\n",
        "    vocab =  sorted(list(set(text)))\n",
        "    print ('{} unique characters'.format(len(vocab)))\n",
        "    return vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIA0F11LQRrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\" Cleans a text. \"\"\"\n",
        "\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"।\", \" \")\n",
        "    text = text.replace(\"0\", \" \")\n",
        "    text = text.replace(\"1\", \" \")\n",
        "    text = text.replace(\"2\", \" \")\n",
        "    text = text.replace(\"3\", \" \")\n",
        "    text = text.replace(\"4\", \" \")\n",
        "    text = text.replace(\"5\", \" \")\n",
        "    text = text.replace(\"6\", \" \")\n",
        "    text = text.replace(\"7\", \" \")\n",
        "    text = text.replace(\"8\", \" \")\n",
        "    text = text.replace(\"9\", \" \")\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xBNrdVKWF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(vocab,text):\n",
        "  #vectorizing the text\n",
        "  # Creating a mapping from unique characters to indices\n",
        "  char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "  idx2char = np.array(vocab)\n",
        "\n",
        "  text_as_int = np.array([char2idx[c] for c in text])\n",
        "  print('{')\n",
        "  for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "  print('  ...\\n}')\n",
        "  # Show how the first 13 characters from the text are mapped to integers\n",
        "  print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "  return text_as_int,idx2char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4uhtHmOSPWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "def char_to_seq(text,text_as_int):\n",
        "  seq_length = 100\n",
        "  examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "  # Create training examples / targets\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "  for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEi-A-eMTwEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "b22ba328-14db-4b57-85a0-2fc8ef3518f5"
      },
      "source": [
        "ensure_corpus()\n",
        "paths = read_file()\n",
        "load_file(paths)\n",
        " # Getting character set.\n",
        "print(\"Getting character set...\")\n",
        "global full_text\n",
        "full_text = \" \".join(file_contents)\n",
        "global character_set\n",
        "character_set = get_character_set(full_text)\n",
        "print(\"Character set:\", character_set, len(character_set))\n",
        "text_int,idx2char = vectorize_text(character_set,full_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading corpus...\n",
            "Unzipping corpus...\n",
            "Display all text files \n",
            "['corpus/hindi_text_ltrc-master/Rahima/main.txt', 'corpus/hindi_text_ltrc-master/Kabeera/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/Uddhav/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/gandhi/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/skgt/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/kamayani/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/vaktavya/main.txt', 'corpus/hindi_text_ltrc-master/miscellaneous/seva/main.txt', 'corpus/hindi_text_ltrc-master/cakra/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/KV/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/5/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/6/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/1/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/2/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/3/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/7/main.txt', 'corpus/hindi_text_ltrc-master/tulasidaas/Raamacharita_maanasa/4/main.txt', 'corpus/hindi_text_ltrc-master/Meera/bjnm/main.txt', 'corpus/hindi_text_ltrc-master/Meera/bjn/main.txt']\n",
            "Loading all files...\n",
            "Getting character set...\n",
            "73 unique characters\n",
            "Character set: [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', ':', '=', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'y', 'z', '|', 'Ê', 'Ù', 'Ý', 'è', 'é', 'ê', 'í', 'ñ', 'û', 'ā', 'ġ', 'ī', 'ś', 'ū', '̐', '़', 'ॅ', 'ॉ', 'फ़', 'ḍ', 'ḥ', 'ḷ', 'ḻ', 'ṃ', 'ṅ', 'ṇ', 'ṛ', 'ṝ', 'ṣ', 'ṭ', '\\u200c', '\\u200d', '–', '’'] 73\n",
            "{\n",
            "  ' ' :   0,\n",
            "  '!' :   1,\n",
            "  '\"' :   2,\n",
            "  \"'\" :   3,\n",
            "  '(' :   4,\n",
            "  ')' :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '/' :   9,\n",
            "  ':' :  10,\n",
            "  '=' :  11,\n",
            "  '?' :  12,\n",
            "  '`' :  13,\n",
            "  'a' :  14,\n",
            "  'b' :  15,\n",
            "  'c' :  16,\n",
            "  'd' :  17,\n",
            "  'e' :  18,\n",
            "  'f' :  19,\n",
            "  ...\n",
            "}\n",
            "'uttama jāti h' ---- characters mapped to int ---- > [34 33 33 14 26 14  0 23 48 33 22  0 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkElQulCU12D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "37789b9c-aacd-41cf-c5aa-89f5e1ab1e5c"
      },
      "source": [
        "text_int,idx2char"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([34, 33, 33, ..., 27, 48, 38]),\n",
              " array([' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', ':', '=', '?',\n",
              "        '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
              "        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'y', 'z', '|',\n",
              "        'Ê', 'Ù', 'Ý', 'è', 'é', 'ê', 'í', 'ñ', 'û', 'ā', 'ġ', 'ī', 'ś',\n",
              "        'ū', '̐', '़', 'ॅ', 'ॉ', 'फ़', 'ḍ', 'ḥ', 'ḷ', 'ḻ', 'ṃ', 'ṅ', 'ṇ',\n",
              "        'ṛ', 'ṝ', 'ṣ', 'ṭ', '\\u200c', '\\u200d', '–', '’'], dtype='<U1'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([34, 33, 33, ..., 27, 48, 38]),\n",
              " array([' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', ':', '=', '?',\n",
              "        '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
              "        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'y', 'z', '|',\n",
              "        'Ê', 'Ù', 'Ý', 'è', 'é', 'ê', 'í', 'ñ', 'û', 'ā', 'ġ', 'ī', 'ś',\n",
              "        'ū', '̐', '़', 'ॅ', 'ॉ', 'फ़', 'ḍ', 'ḥ', 'ḷ', 'ḻ', 'ṃ', 'ṅ', 'ṇ',\n",
              "        'ṛ', 'ṝ', 'ṣ', 'ṭ', '\\u200c', '\\u200d', '–', '’'], dtype='<U1'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN8QZhEqOix4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "db887557-009e-4642-afa4-4c053f27e0f5"
      },
      "source": [
        "print((text_int))\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(full_text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34 33 33 ... 27 48 38]\n",
            "u\n",
            "t\n",
            "t\n",
            "a\n",
            "m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6KUYvHvVP2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0d74c43c-3850-47fd-c85c-014fd93ea6eb"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala meṃ\n",
            "'uttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala meṃ'\n",
            " harata, parasata vāke pāya rūparaṃga ratirāja meṃ, chatarānī itarāna| mānau racī biraṃci paci, kusum\n",
            "' harata, parasata vāke pāya rūparaṃga ratirāja meṃ, chatarānī itarāna| mānau racī biraṃci paci, kusum'\n",
            "a kanaka meṃ sāna baniyāini bani āikai, baiṭhi rūpa kī hāṭa| pema peka tana herikai, garuvai ṭārati b\n",
            "'a kanaka meṃ sāna baniyāini bani āikai, baiṭhi rūpa kī hāṭa| pema peka tana herikai, garuvai ṭārati b'\n",
            "āṭa garaba tarājū karati cakha, bhauṃha mori musakāti| ḍām̐fī mārati biraha kī, cita ciṃtā ghaṭi jāti\n",
            "'āṭa garaba tarājū karati cakha, bhauṃha mori musakāti| ḍām̐fī mārati biraha kī, cita ciṃtā ghaṭi jāti'\n",
            " kamala-dala nainani kī unamāni / rahīma kamala-dala nainani kī unamāni| bisarata nāhiṃ sakhī mo mana\n",
            "' kamala-dala nainani kī unamāni / rahīma kamala-dala nainani kī unamāni| bisarata nāhiṃ sakhī mo mana'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt7soWYMVAYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPS02POgVVFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "51ffba02-6468-4ac6-caae-d8863eff479d"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'uttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala me'\n",
            "Target data: 'ttama jāti hai bāhmanī / rahīma uttama jāti hai bāhmanī, dekhata citta lubhāya| parama pāpa pala meṃ'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk0BwKToVis6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e368e18-341c-424b-f4fe-e8a3c446259b"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0srsOmu8VoXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(character_set)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC83vIZwWH4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Df6nq0WKdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(character_set),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ4mR7iaWP4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6b0918e-88f1-4591-e47e-a5fa1823b03d"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 73) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfUrNjGrWSsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e0b6e251-ecd4-470f-8bb4-2adfa46f2b97"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           18688     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 73)            74825     \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_rniZB_WVQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I29PLE0gWXfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f9f112ad-df0d-4f27-dd9c-f5e716705639"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21,  3, 62, 10, 30, 25, 54, 19, 11, 18, 44, 67, 38, 60,  4, 58,  4,\n",
              "       72, 39, 62, 55, 49, 36, 27, 11, 48, 56, 59, 46, 42,  0,  3,  8, 61,\n",
              "       29, 10, 65,  3,  2, 70, 69,  8, 11, 46, 16, 52, 61, 47, 55, 26, 32,\n",
              "       36, 24, 72, 58, 24, 27, 12,  7, 58, 11, 72, 33, 19, 35, 63, 71, 17,\n",
              "       64, 39, 47, 69, 11, 39, 30, 35, 23, 62,  7, 70, 60,  4,  6, 68, 17,\n",
              "       46, 11, 45, 32, 54,  3, 11, 41, 40, 42, 49,  2, 47, 14,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42, 29, 31, 37, 66,  9, 49, 65, 11, 44, 60, 31, 60,  0, 72,  6, 12,\n",
              "        1,  9, 18, 55, 27, 70,  2, 49, 43, 71, 17, 19, 41, 59,  4, 66, 40,\n",
              "        9, 58, 50, 20, 45, 48,  9, 60, 25, 69, 28, 18, 34, 56, 36, 65,  9,\n",
              "       50, 45, 29, 21,  3, 41, 65,  9, 59, 69,  1, 53,  9, 25, 52, 67, 65,\n",
              "       29, 70, 61, 15, 41, 20, 47, 67, 33,  6, 21, 10, 29, 69, 67, 65,  1,\n",
              "       48, 35, 62, 66,  8, 42,  4, 47, 34, 49,  6, 28, 17, 31,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO31QFlNWaCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "845e42b5-8c7b-4747-f442-a871d2d8a87c"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "temp = ''.join(idx2char[sampled_indices])\n",
        "new_text = sanscript.transliterate(temp, sanscript.DEVANAGARI, sanscript.IAST)\n",
        "print(new_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'pīche r pīche ātī thī . kabhī sāmane ākara rāstā roka letī aura kahatīr merī yaha durgati tumane kī '\n",
            "\n",
            "Next Char Predictions: \n",
            "\n",
            "=èsciébṭñ‌ê‍ṅû|ॅÙ=v|b=ī–फ़ûûś,stṣġ’Ýṅi:síḷ:iūÝdÊ(Ùṝūṅॉ़lpi‍`gṝ\"gḻmirna)yś̐ke||‍dफ़ciṣṛ!lfफ़ā,ūl`:b`फ़i=ॅ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwocxBEgYBoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "86abdbac-c43f-4c15-c8b5-dd3f8e7da107"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 73)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.2920275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CncARvwBYEMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fT6a4eYG3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU3cMW6WYJbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "160838a8-971b-41f5-e766-22c8b6622d01"
      },
      "source": [
        "EPOCHS=10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 35/455 [=>............................] - ETA: 30:34 - loss: 3.3493"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Am5KKBdctzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6XLc4MgJZLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    \"\"\" Loads the data from the corpus. \"\"\"\n",
        "\n",
        "\n",
        "    # Getting character set.\n",
        "    print(\"Getting character set...\")\n",
        "    global full_text\n",
        "    full_text = \" \".join(file_contents)\n",
        "    global character_set\n",
        "    character_set = get_character_set(full_text)\n",
        "    print(\"Character set:\", character_set, len(character_set))\n",
        "\n",
        "    # Process the data.\n",
        "    data_input = []\n",
        "    data_output = []\n",
        "    current_size = 0\n",
        "    print(\"Generating data set...\")\n",
        "    bar = progressbar.ProgressBar(max_value=data_set_size)\n",
        "    while current_size < data_set_size:\n",
        "        random_file_content = random.choice(file_contents)\n",
        "\n",
        "        random_string = random_substring_of_length(random_file_content, input_length + output_length)\n",
        "\n",
        "        random_string_encoded = encode_string(random_string)\n",
        "\n",
        "        input_sequence = random_string_encoded[:input_length]\n",
        "        output_sequence = random_string_encoded[input_length:]\n",
        "\n",
        "        data_input.append(input_sequence)\n",
        "        data_output.append(output_sequence)\n",
        "\n",
        "        current_size += 1\n",
        "        bar.update(current_size)\n",
        "    bar.finish()\n",
        "\n",
        "    # Done.\n",
        "    train_input = np.array(data_input)\n",
        "    train_output = np.array(data_output)\n",
        "    return (train_input, train_output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}